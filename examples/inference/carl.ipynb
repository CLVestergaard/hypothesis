{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hypothesis\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch.nn.utils import spectral_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_true = torch.tensor(1, dtype=torch.float)\n",
    "\n",
    "normal = Normal(mean_true, 1.)\n",
    "lower = torch.tensor(0, dtype=torch.float)\n",
    "upper = torch.tensor(2, dtype=torch.float)\n",
    "prior = Uniform(lower, upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator(theta):\n",
    "    normal = Normal(theta, 1)\n",
    "    return normal.sample().view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypothesis.nn import ParameterizedClassifier\n",
    "from hypothesis.nn import ParameterizedClassifierEnsemble\n",
    "\n",
    "def allocate_classifier(hidden=256):\n",
    "    classifier = torch.nn.Sequential(\n",
    "        torch.nn.Linear(2, hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden, hidden),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden, 1),\n",
    "        torch.nn.Sigmoid())\n",
    "    classifier = ParameterizedClassifier(classifier, lower=lower, upper=upper)\n",
    "    classifier.train()\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypothesis.io.dataset import GeneratorDataset as Dataset\n",
    "\n",
    "dataset = Dataset(simulator, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypothesis.train import ParameterizedClassifierTrainer as Trainer\n",
    "\n",
    "def train():\n",
    "    batch_size = 512\n",
    "    classifier = allocate_classifier()\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.0001)\n",
    "    trainer = Trainer(classifier, dataset, optimizer, shuffle=True, epochs=30, batch_size=batch_size, criterion=torch.nn.BCELoss(reduction=\"mean\"))\n",
    "    trainer.train()\n",
    "    classifier.eval()\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier 1\n",
      "Training classifier 2\n"
     ]
    }
   ],
   "source": [
    "num_classifiers = 2\n",
    "\n",
    "classifiers = []\n",
    "for classifier_index in range(num_classifiers):\n",
    "    print(\"Training classifier\", classifier_index + 1)\n",
    "    classifier = train()\n",
    "    classifiers.append(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = ParameterizedClassifierEnsemble(classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.eval()\n",
    "thetas = torch.arange(lower, upper, 0.01)\n",
    "samples = 256\n",
    "observations = normal.sample(torch.Size([samples]))\n",
    "\n",
    "ratios = []\n",
    "for theta in thetas:\n",
    "    theta = theta.repeat(samples)\n",
    "    ratio = ensemble.forward_all(observations, theta).view(samples, -1).mean(dim=0).view(1, -1)\n",
    "    ratios.append(ratio)\n",
    "\n",
    "ratios = torch.cat(ratios, dim=0)\n",
    "# Plot the estimated density function.\n",
    "plt.plot(thetas.detach().numpy(), ratios.detach().numpy())\n",
    "plt.axvline(observations.mean().numpy())\n",
    "#plt.ylim([0, None])\n",
    "plt.show()\n",
    "\n",
    "m_ratio = ratios.mean(dim=1).detach()\n",
    "plt.plot(thetas.detach().numpy(), m_ratio.numpy())\n",
    "plt.show()\n",
    "\n",
    "value, index = m_ratio.min(0)\n",
    "print(thetas[index])\n",
    "print(observations.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.eval()\n",
    "thetas = torch.arange(lower, upper, 0.01)\n",
    "samples = 256\n",
    "#observations = normal.sample(torch.Size([samples]))\n",
    "\n",
    "ratios = []\n",
    "for theta in thetas:\n",
    "    ratio = -ensemble.log_likelihood_to_evidence_ratio(observations, theta).view(-1)\n",
    "    ratios.append(ratio)\n",
    "    \n",
    "ratios = torch.cat(ratios, dim=0).exp()\n",
    "ratios /= ratios.sum()\n",
    "\n",
    "plt.plot(thetas.detach().numpy(), ratios.detach().numpy())\n",
    "#plt.axvline(observations.mean())\n",
    "plt.axvline(mean_true)\n",
    "#plt.ylim([0, None])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
